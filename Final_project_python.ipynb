{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11fcd0d-f36f-4ec9-aece-4ad1333ca8b2",
   "metadata": {},
   "source": [
    "# Data Extraction, Tranformation and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6983a-1680-4fe9-9115-fbc9905a1845",
   "metadata": {},
   "source": [
    "# 1. Instructional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c736f4da-ecc1-4fa1-841b-27e6189d558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/SAL2019_IS.zip...\n",
      "Extracting sal2019_is.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/SAL2020_IS.zip...\n",
      "Extracting sal2020_is.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/SAL2021_IS.zip...\n",
      "Extracting sal2021_is.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/SAL2022_IS.zip...\n",
      "Extracting sal2022_is.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/SAL2023_IS.zip...\n",
      "Extracting sal2023_is.csv...\n",
      "Index(['UNITID', 'ARANK', 'Year', 'Item', 'Value'], dtype='object')\n",
      "Existing data deleted from instruction table.\n",
      "Number of rows in instruction table after truncation: 0\n",
      "Data appended to PostgreSQL table instruction\n",
      "Number of rows in instruction table after appending data: 1865624\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Define base URL and range of years\n",
    "base_url = \"https://nces.ed.gov/ipeds/datacenter/data/\"\n",
    "years = range(2019, 2024)  # From 2019 to 2023\n",
    "\n",
    "# Columns to unpivot (if they exist)\n",
    "columns_to_melt = [\"SA_9MCM\", \"SA09MCM\", \"SA10MCM\", \"SA11MCM\", \"SA12MCM\",\n",
    "        \"SA09MOM\", \"SA10MOM\", \"SA11MOM\", \"SA12MOM\",\n",
    "        \"SAEQ9OM\", \"SAEQ9AM\", \"SA09MAM\", \"SA10MAM\", \"SA11MAM\", \"SA12MAM\",\n",
    "        \"SA_9MCW\", \"SA09MCW\", \"SA10MCW\", \"SA11MCW\", \"SA12MCW\",\n",
    "        \"SA09MOW\", \"SA10MOW\", \"SA11MOW\", \"SA12MOW\",\n",
    "        \"SAEQ9OW\", \"SAEQ9AW\", \"SA09MAW\", \"SA10MAW\", \"SA11MAW\", \"SA12MAW\"]\n",
    "\n",
    "# Create an empty list to store data\n",
    "all_data = []\n",
    "\n",
    "# Loop through each year, download, extract, and process the file\n",
    "for year in years:\n",
    "    zip_url = f\"{base_url}SAL{year}_IS.zip\"\n",
    "    print(f\"Downloading {zip_url}...\")\n",
    "\n",
    "    # Download the ZIP file\n",
    "    response = requests.get(zip_url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(BytesIO(response.content), \"r\") as z:\n",
    "            # Find the CSV file inside the ZIP (assuming only one CSV per ZIP)\n",
    "            csv_filename = [f for f in z.namelist() if f.endswith(\".csv\")][0]\n",
    "            print(f\"Extracting {csv_filename}...\")\n",
    "\n",
    "            # Read CSV file directly from ZIP\n",
    "            with z.open(csv_filename) as f:\n",
    "                df = pd.read_csv(f, dtype=str)  # Ensure all columns are read as strings\n",
    "\n",
    "            # Add Year column\n",
    "            df[\"Year\"] = year\n",
    "\n",
    "            # Only keep columns that exist in the dataset\n",
    "            available_columns = [\"UNITID\", \"ARANK\"] + [col for col in columns_to_melt if col in df.columns]\n",
    "            df = df[available_columns + [\"Year\"]]\n",
    "\n",
    "            # Unpivot data (Convert Wide to Long Format)\n",
    "            df_melted = df.melt(id_vars=[\"UNITID\", \"ARANK\", \"Year\"], \n",
    "                                var_name=\"Item\", \n",
    "                                value_name=\"Value\")\n",
    "\n",
    "            # Append to list\n",
    "            df_melted[\"Value\"] = pd.to_numeric(df_melted[\"Value\"], errors='coerce')\n",
    "            all_data.append(df_melted)\n",
    "    else:\n",
    "        print(f\"Failed to download {zip_url}\")\n",
    "\n",
    "# Combine all years into one DataFrame\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(df.columns)  # Check column names\n",
    "\n",
    "# Drop rows with NaN values in the Value column\n",
    "final_df = df.dropna(subset=['Value'])\n",
    "\n",
    "# Database connection details\n",
    "db_url = \"postgresql://rankings:xxxx/rankings?sslmode=require\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Delete all content from the existing table\n",
    "with engine.connect() as connection:\n",
    "    # Truncate the table\n",
    "    connection.execute(text(\"TRUNCATE TABLE instruction;\"))\n",
    "    print(\"Existing data deleted from instruction table.\")\n",
    "\n",
    "    # Verify that the table is empty\n",
    "    result = connection.execute(text(\"SELECT COUNT(*) FROM instruction;\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"Number of rows in instruction table after truncation: {count}\")\n",
    "\n",
    "    if count != 0:\n",
    "        raise Exception(\"Truncate command did not execute correctly. Table is not empty.\")\n",
    "\n",
    "# Save DataFrame to PostgreSQL\n",
    "table_name = \"instruction\"\n",
    "final_df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "print(f\"Data appended to PostgreSQL table {table_name}\")\n",
    "\n",
    "# Verify the number of rows after appending data\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT COUNT(*) FROM instruction;\"))\n",
    "    count_after_append = result.scalar()\n",
    "    print(f\"Number of rows in instruction table after appending data: {count_after_append}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eea09b-f53b-41a8-97b6-6ab6bb37f53f",
   "metadata": {},
   "source": [
    "# 2.Library data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaed60cd-fb4d-4b50-a37d-e0a58807d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/AL2019.zip...\n",
      "Extracting al2019.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/AL2020.zip...\n",
      "Extracting al2020.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/AL2021.zip...\n",
      "Extracting al2021.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/AL2022.zip...\n",
      "Extracting al2022.csv...\n",
      "Downloading https://nces.ed.gov/ipeds/datacenter/data/AL2023.zip...\n",
      "Extracting al2023.csv...\n",
      "Index(['UNITID', 'LCOLELYN', 'Year', 'Item', 'Value'], dtype='object')\n",
      "Existing data deleted from library table.\n",
      "Number of rows in library table after truncation: 0\n",
      "Data appended to PostgreSQL table library\n",
      "Number of rows in library table after appending data: 143336\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Define base URL and range of years\n",
    "base_url = \"https://nces.ed.gov/ipeds/datacenter/data/\"\n",
    "years = range(2019, 2024)  # From 2019 to 2023\n",
    "\n",
    "# Columns to unpivot (if they exist)\n",
    "columns_to_melt = ['LPBOOKS', 'LEBOOKS', 'LEDATAB', 'LPMEDIA', 'LEMEDIA', 'LPSERIA', 'LESERIA', 'LEXOMTL']\n",
    "\n",
    "# Create an empty list to store data\n",
    "all_data = []\n",
    "\n",
    "# Loop through each year, download, extract, and process the file\n",
    "for year in years:\n",
    "    zip_url = f\"{base_url}AL{year}.zip\"\n",
    "    print(f\"Downloading {zip_url}...\")\n",
    "\n",
    "    # Download the ZIP file\n",
    "    response = requests.get(zip_url)\n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(BytesIO(response.content), \"r\") as z:\n",
    "            # Find the CSV file inside the ZIP (assuming only one CSV per ZIP)\n",
    "            csv_filename = [f for f in z.namelist() if f.endswith(\".csv\")][0]\n",
    "            print(f\"Extracting {csv_filename}...\")\n",
    "\n",
    "            # Read CSV file directly from ZIP\n",
    "            with z.open(csv_filename) as f:\n",
    "                df = pd.read_csv(f, dtype=str)  # Ensure all columns are read as strings\n",
    "\n",
    "            # Add Year column\n",
    "            df[\"Year\"] = year\n",
    "\n",
    "            # Only keep columns that exist in the dataset\n",
    "            available_columns = [\"UNITID\", \"LCOLELYN\"] + [col for col in columns_to_melt if col in df.columns]\n",
    "            df = df[available_columns + [\"Year\"]]\n",
    "\n",
    "            # Unpivot data (Convert Wide to Long Format)\n",
    "            df_melted = df.melt(id_vars=[\"UNITID\", \"LCOLELYN\", \"Year\"], \n",
    "                                var_name=\"Item\", \n",
    "                                value_name=\"Value\")\n",
    "\n",
    "            # Append to list\n",
    "            df_melted[\"Value\"] = pd.to_numeric(df_melted[\"Value\"], errors='coerce')\n",
    "            all_data.append(df_melted)\n",
    "    else:\n",
    "        print(f\"Failed to download {zip_url}\")\n",
    "\n",
    "# Combine all years into one DataFrame\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(df.columns)  # Check column names\n",
    "\n",
    "# Drop rows with NaN values in the Value column\n",
    "final_df = df.dropna(subset=['Value'])\n",
    "\n",
    "# Database connection details\n",
    "db_url = \"postgresql://rankings:xxxx/rankings?sslmode=require\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Delete all content from the existing table\n",
    "with engine.connect() as connection:\n",
    "    # Truncate the table\n",
    "    connection.execute(text(\"TRUNCATE TABLE library;\"))\n",
    "    print(\"Existing data deleted from library table.\")\n",
    "\n",
    "    # Verify that the table is empty\n",
    "    result = connection.execute(text(\"SELECT COUNT(*) FROM library;\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"Number of rows in library table after truncation: {count}\")\n",
    "\n",
    "    if count != 0:\n",
    "        raise Exception(\"Truncate command did not execute correctly. Table is not empty.\")\n",
    "\n",
    "# Save DataFrame to PostgreSQL\n",
    "table_name = \"library\"\n",
    "final_df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "print(f\"Data appended to PostgreSQL table {table_name}\")\n",
    "\n",
    "# Verify the number of rows after appending data\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT COUNT(*) FROM library;\"))\n",
    "    count_after_append = result.scalar()\n",
    "    print(f\"Number of rows in library table after appending data: {count_after_append}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4543c5-3919-424c-a7ae-b9cad62eda2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
